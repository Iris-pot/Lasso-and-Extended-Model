[TOC]

# Lasso基础理论

​		在经典线性回归中，若变量维度$p$远远大于样本数量$n$ ，则最小二乘估计的结果并不唯一，有无穷多个解可使目标函数为零，而且大多数解都会过拟和数据。导致在总体没有共线性的情况下，样本展现出极强的共线性。


​		由此引入稀疏性与正则化。稀疏性具有三个明显优势：模型可以解释、计算简单，和押注稀疏性（bet on sparsity）。押注稀疏性是指，既然无法有效处理稠密问题，倒不如在稀疏问题上寻找有效的处理方法。

## 1 经典Lasso

## 1.1 Lasso和Ridge模型

​		Lasso与Ridge回归的出现是为了解决线性回归出现的过拟合问题，这两种回归均在标准线性回归的基础上，通过在损失函数中引入正则化项来达到目的。其中，Lasso使用$L1$范数，Ridge使用$L2$范数。

​		将Lasso写成如下约束模式：
$$
J(\theta)=\frac{1}{2}\sum_{i}^{m}(y^{(i)}-\theta ^Tx^{(i)})^2+\lambda \sum_{j}^{n}|\theta_j|
$$
​		将Ridge写成如下约束模式：
$$
J(\theta)=\frac{1}{2}\sum_{i}^{m}(y^{(i)}-\theta ^Tx^{(i)})^2+\lambda \sum_{j}^{n}\theta_j^2
$$

## 1.2 Lasso的估计及局限

​		绘制了Lasso、Ridge和最小二乘法的参数估计图，图中黑色线代表最小二乘法的参数估计值，直线斜率最高，参数估计值最大；蓝色线代表Ridge的参数估计值，直线斜率低于最小二乘法估计，即Ridge做到全局缩放；红色线代表Lasso的参数估计值，在绝对值小于时$\lambda/2$，Lasso使其直接为0，绝对值大于$\lambda/2$时，Lasso使其缩减一定程度。因此Lasso满足了稀疏性，可以产生稀疏系数。

![image-20200605160511685](C:\Users\zmy\AppData\Roaming\Typora\typora-user-images\image-20200605160511685.png)

​		由此也可以看出Lasso的局限性，即当参数估计值很大时，认为该参数很重要，Lasso的估计仍然将其缩减，对系数压缩程度较大，导致估计偏差较大，即不满足无偏性。同时， 如果自变量之间存在多重共线时， Lasso的估计效果会变得很差。

## 2 Adaptive Lasso

​		为解决Lasso估计偏差较大的问题，Zou (2006)提出了Adaptive Lasso的方法以进行改进，基本思想是对不同的系数赋予不同的压缩权重。对不同的变量系数                                运用不同的调整参数，下面给出Adaptive Lasso的定义：
$$
\hat{\beta} = {\arg\limits_{\beta}{{\min\{ \frac{1}{2} }| | Y - X\beta{||}_{2}^{2} + \lambda{\sum\limits_{j = 1}^{p}{w_{j}| \beta_{j} | \}~}} }}
$$
​		其中权重向量$w_{j}$的选取方式是，假设$\widehat {\beta _{j}}$是真实模型$\beta _{j}$的一个估计，$\widehat {\beta _{j}}$采用最小二乘法估计、Lasso估计、Ridge估计等估计值作为$\beta _{j}$的初步参照值，再选取一个参数$\nu$，则权重向量定义为$w_{j}=\dfrac {1}{\left| \widehat  {\beta _{j}}\right| ^{\nu }}$。

​		当$\widehat {\beta _{j}}$为0时，会推导出$\beta _{j}$为0。而且$\widehat {\beta _{j}}$越大，则Adaptive Lasso对第$j$个参数  施加的惩罚越小，从而偏差也会较小。所以Adaptive lasso也会得到稀疏解，并且会减小不相关变量被选中的概率。

​		值得指出的是，Adaptive Lasso是一个凸优化问题，这样它就不会陷入局部最优解。

## 3 惩罚函数特征

​		稀疏性限制条件会使部分变量的系数收缩至零，相当于挑选了一部分重要的变量进入随后的分析，此所谓变量选择。变量选择是传统的最佳子集选择的延伸，在现代统计学习中扮演着重要角色。稀疏性罚函数的研究与变量选择问题息息相关.

​		针对稀疏性罚函数的设计，Fan和Lv（2010）提出了3个重要标准，不过没有一个范数恰好能同时满足这3个标准。

​		第一，无偏性：估计器是近似无偏的，尤其是当实际系数的值较大时，估计器的无偏性能够降低模型的偏差，通常使用的$L1$惩罚函数并不满足无偏性；

​		第二，稀疏性：估计器自动将小的系数压缩至零，实现变量选择，对于范数$Lq$，当$q>1$时不满足稀疏性标准；

​		第三，连续性：估计器具备连续性，可以降低模型预测的不稳定性，对于范数$Lq$来说，当$1>q≥0$时不具备连续性。

​		Lasso在$L1$惩罚下满足稀疏性和连续性，但不满足无偏性。当估计的参数值比较小的时候，Lasso使其直接等于0，满足了稀疏性的条件，并且图像是连续的，也满足了连续性的条件。但是对于那些估计出来比较大的参数，并不具有无偏性。其中最小二乘法的系数估计为无偏估计，而Lasso估计的出来系数一直与其相差了一个$\lambda$，使得Lasso不满足上述所谈到的一个好的惩罚函数所具有的无偏性。

## 4 SCAD

​		Fan和L(2001)改进了Lasso法，提出了SCAD(the Smoothly Clipped Absolute Deviation Penalty)方法，其惩罚函数是对称非凹的，并被证明了满足无偏性、稀疏性和连续性的性质。同时，SCAD比Lasso更为稳定，并且大大降低了计算量。

​		SACD的数学形式为：
$$
\hat{\beta} = {\arg\limits_{\beta}{\min| | y - {\sum\limits_{j = 1}^{p} X_{j}\beta_{j} | |_{2}^{2}~ } + ~{\sum\limits_{j = 1}^{p}{P_{\lambda}( | \beta _{j} | )}} }}
$$
​		SCAD方法通过惩罚函数，将较小的系数压缩变小，甚至等于0；而对于较大的系数基本保持不变，从而实现了变量选择的目的。同时文献中还证明了该方法所得估计量满足无偏性、稀疏性、连续性和Oracle等优良性质。

​		在具体计算中，由于该模型含有两个参数$\lambda$和$\alpha$，因此计算过程相对复杂，需要通过二维广义交叉验证的方法来实现，因此Fan指出，通常$\alpha$取值为3.7的值可以通过交叉验证法解得。

## 5 Elastic Net

​		Lasso和Adaptive Lasso方法尽管在很多情形下具有良好的性质，但是它们具有如下的局限性：

​		第一，由于凸优化问题的本质，在$p>n$的情形下，Lasso和Adaptive lasso最多只能选择$n$个变量。

​		第二，如果有两个变量具有高度的相关性，Lasso和Adaptive lasso通常只会随机选择其中的一个。诚然，这会让模型更加稀疏。但是，如果一个“假的”自变量和一个“真的”自变量具有很高的相关度，Lasso和Adaptive lasso可能会选择这个“假的”自变量而舍弃“真的”自变量。

​		第三，在常规$n>p$的情形下，如果变量之间具有很高的共线性，则Lasso和Adaptive lasso的表现非常差。

​		第一条和第二条的局限性使得Lasso和Adaptive lasso在某些问题中就变得不是很适用了，因此Zou和Hastie（2005）提出了新的弹性网（Elastic net）方法来解决上面的局限性。

​		同时使用L1和L2惩罚，其形式为：
$$
J(\theta)=\frac{1}{2}\sum_{i}^{m}(y^{(i)}-\theta ^Tx^{(i)})^2+\lambda (\rho\sum_{j}^{n}|\theta_j|+(1-\rho)\sum_{j}^{n}\theta_j^2)
$$
​		Elastic Net是Lasso和岭回归的结合体，同时具有Lasso的是回归系数变得稀疏的性质，也具有岭回归那样在高维计算中的稳健性。弹性网可以使得在优化过程中，相关性较强的变量的系数较为接近，即产生“组效应”。

​		它在Lasso与岭回归之间做了折中：加上一个调整参数$\rho$，使得其兼具二者的性质，惩罚项中的$1/2$是一个更加直观的软阀值因子。当$\rho=1$时，Elastic Net模型退化为Lasso模型；当$\rho=0$时，其变为岭回归模型。

## 6 Group Lasso

​		在某些线性模型中，会存在某几个变量的系数同时为0或同时不为0的现象。Yuan和Lin（2006）针对上述现象提出了组Lasso（Group Lasso）的概念：将同时为0或不为0的变量归为一组，通过在目标函数中惩罚每一组的$L2$范数，可以将一整组的系数同时消成零，即抹掉一整组的变量。用新的变量$X_{J}$表示，其系数为$\beta_{j}$,模型可以表示为：
$$
\hat{\beta} = {\arg\limits_{\beta}{{\min\{ \frac{1}{2n} }| | Y - X\beta{||}_{2}^{2} + \lambda{\sum\limits_{j = 1}^{p}{\| \beta_{j} \|_{2} \}~}} }}
$$
​		通过压缩组内变量的系数平方和来实现变量的选择，并通过变量组的概念将有内在联系的数据作为一个整体进行分析，成功将组Lasso方法推广到机器学习和数据挖掘等领域。

​		容易看出，Group Lasso是对Lasso的一种推广，即对特征分组后的Lasso。显然，如果每个组的特征个数都是1，则Group Lasso就退化为原始的Lasso。为了求解Group Lasso，可以首先假设组内特征是正交的，针对这种情形可以利用分块坐标下降法求解，对于非正交的情形，可以首先对组内特征施加正交化。

​		不同于Lasso 方法将每个特征的系数项的绝对值加总，Group Lasso所加总的是每个组系数的 L2 范数，在优化的过程中，该结构尽量选出更少的组（组间稀疏），而组内是L2范数，稀疏约束没那么强。

​		Raman（2009）针对Group Lasso在回归系数的方差上不具有可解释性这一缺点,将Group Lasso推广为贝叶斯组Lasso，并在生物医学领域进行应用，取得了很好的效果。Friedman（2010）将Group Lasso和Lasso相结合，提出了稀疏组Lasso （Sparse Group Lasso）模型，通过组内和组间两次的变量选择进一步剔除无关变量，克服了特征组内可能存在冗余变量的缺点，从而实现更有效的变量降维，因此在实际问题中有着广泛的应用。

​		此外，基于Group Lasso方法的相关模型都是建立在这样的假设条件下：组间变量无关而组内变量存在相关性，也就是并没有考虑重叠组的问题。为了进一步对包含重叠组变量的问题进行研究，Zhao等（2009）将具有嵌套结构的Group Lasso惩罚项作为最小二乘的惩罚函数，提出了分层组Lasso（Hierarchical Group Lasso）模型。

